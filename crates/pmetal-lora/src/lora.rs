//! Standard LoRA (Low-Rank Adaptation) implementation.
//!
//! LoRA adds low-rank trainable matrices to frozen pretrained weights:
//! `y = x @ W.T + scale * (x @ A.T) @ B.T`
//!
//! Where:
//! - `W` is the frozen base weight matrix
//! - `A` is the LoRA down-projection matrix (rank x in_features)
//! - `B` is the LoRA up-projection matrix (out_features x rank)
//! - `scale = alpha / rank` (or `alpha / sqrt(rank)` for RSLoRA)
//!
//! ## Optimized Mode
//!
//! When `use_optimized` is enabled, LoRA uses pre-transposed and pre-scaled
//! matrices for ~30% faster forward passes:
//! - No transpose at forward time (pre-computed)
//! - Scale pre-baked into B matrix
//!
//! The optimized cache is automatically invalidated after gradient updates
//! and lazily rebuilt on next forward pass.

use mlx_rs::{error::Exception, nn, Array};

use pmetal_core::LoraConfig;
use pmetal_mlx::kernels::fast_lora::{optimized_lora_forward, OptimizedLoraParams};

/// Error type for LoRA operations.
#[derive(Debug, thiserror::Error)]
pub enum LoraError {
    /// MLX error.
    #[error("MLX error: {0}")]
    Mlx(#[from] Exception),
    /// IO error.
    #[error("IO error: {0}")]
    Io(#[from] mlx_rs::error::IoError),
    /// Shape mismatch error.
    #[error("Shape mismatch: {0}")]
    ShapeMismatch(String),
    /// Invalid state error.
    #[error("Invalid state: {0}")]
    InvalidState(String),
}

/// LoRA Linear layer that wraps a base Linear layer with low-rank adaptation.
///
/// Implements: `y = x @ W.T + scale * (x @ A.T) @ B.T`
///
/// When `use_optimized` is true, uses pre-transposed matrices for ~30% faster forward.
#[derive(Debug)]
pub struct LoraLinear {
    /// Input features dimension.
    pub in_features: i32,
    /// Output features dimension.
    pub out_features: i32,
    /// LoRA rank.
    pub rank: i32,
    /// LoRA scaling factor (alpha / rank).
    pub scale: f32,
    /// Whether the layer is merged.
    pub merged: bool,
    /// Whether to use bias.
    pub use_bias: bool,

    /// Frozen base weight matrix [out_features, in_features].
    pub weight: Array,
    /// Optional bias [out_features].
    pub bias: Option<Array>,
    /// LoRA A matrix (rank x in_features) - trainable.
    pub lora_a: Array,
    /// LoRA B matrix (out_features x rank) - trainable.
    pub lora_b: Array,

    /// Whether to use optimized (pre-transposed) forward pass.
    use_optimized: bool,
    /// Cached optimized params (lazily created, invalidated on update).
    optimized_cache: Option<OptimizedLoraParams>,
}

impl LoraLinear {
    /// Create a new LoRA linear layer from a base Linear layer.
    pub fn from_linear(
        linear: &nn::Linear,
        rank: i32,
        alpha: f32,
        use_rslora: bool,
    ) -> Result<Self, LoraError> {
        let weight = linear.weight.value.as_ref();
        let in_features = weight.dim(-1);
        let out_features = weight.dim(-2);

        // Compute scaling factor
        let scale = if use_rslora {
            alpha / (rank as f32).sqrt()
        } else {
            alpha / rank as f32
        };

        // Initialize LoRA A with Kaiming uniform
        let bound = (3.0_f32 / in_features as f32).sqrt();
        let lora_a = mlx_rs::random::uniform::<_, f32>(-bound, bound, &[rank, in_features], None)?;

        // Initialize LoRA B with zeros
        let lora_b = mlx_rs::ops::zeros::<f32>(&[out_features, rank])?;

        // Clone bias if present
        let bias = linear.bias.value.as_ref().cloned();

        Ok(Self {
            in_features,
            out_features,
            rank,
            scale,
            merged: false,
            use_bias: bias.is_some(),
            weight: weight.clone(),
            bias,
            lora_a,
            lora_b,
            use_optimized: false, // Disabled - pre-transpose doesn't help since .t() is a view in MLX
            optimized_cache: None,
        })
    }

    /// Create a new LoRA linear layer with given dimensions.
    pub fn new(
        in_features: i32,
        out_features: i32,
        rank: i32,
        alpha: f32,
        use_rslora: bool,
        use_bias: bool,
    ) -> Result<Self, LoraError> {
        // Compute scaling factor
        let scale = if use_rslora {
            alpha / (rank as f32).sqrt()
        } else {
            alpha / rank as f32
        };

        // Initialize base weight with Kaiming uniform
        let bound = (3.0_f32 / in_features as f32).sqrt();
        let weight =
            mlx_rs::random::uniform::<_, f32>(-bound, bound, &[out_features, in_features], None)?;

        // Initialize bias if needed
        let bias = if use_bias {
            Some(mlx_rs::ops::zeros::<f32>(&[out_features])?)
        } else {
            None
        };

        // Initialize LoRA A with Kaiming uniform
        let lora_a = mlx_rs::random::uniform::<_, f32>(-bound, bound, &[rank, in_features], None)?;

        // Initialize LoRA B with zeros
        let lora_b = mlx_rs::ops::zeros::<f32>(&[out_features, rank])?;

        Ok(Self {
            in_features,
            out_features,
            rank,
            scale,
            merged: false,
            use_bias,
            weight,
            bias,
            lora_a,
            lora_b,
            use_optimized: false, // Disabled - pre-transpose doesn't help since .t() is a view in MLX
            optimized_cache: None,
        })
    }

    /// Create from LoraConfig.
    pub fn from_config(
        in_features: i32,
        out_features: i32,
        config: &LoraConfig,
        use_bias: bool,
    ) -> Result<Self, LoraError> {
        Self::new(
            in_features,
            out_features,
            config.r as i32,
            config.alpha,
            config.use_rslora,
            use_bias,
        )
    }

    /// Forward pass through the LoRA linear layer.
    ///
    /// If `use_optimized` is true, uses pre-transposed matrices for ~30% faster forward.
    /// If merged, uses merged weights. Otherwise computes:
    /// `y = x @ W.T + scale * (x @ A.T) @ B.T`
    ///
    /// Note: The optimized cache is rebuilt on each forward pass because optimizer
    /// updates parameters in-place without triggering cache invalidation.
    pub fn forward(&mut self, x: &Array) -> Result<Array, LoraError> {
        if self.merged {
            // Use merged weight directly
            let y = x.matmul(&self.weight.t())?;
            if let Some(ref bias) = self.bias {
                Ok(y.add(bias)?)
            } else {
                Ok(y)
            }
        } else if self.use_optimized {
            // Optimized forward: pre-scale B to eliminate multiply operation
            // y = x @ W.T + (x @ A.T) @ (scale * B).T
            // Note: .t() is a view operation in MLX (nearly free)
            let y_base = x.matmul(&self.weight.t())?;

            // Pre-scale B and compute LoRA contribution
            let scale_arr = Array::from_f32(self.scale);
            let lora_b_scaled = self.lora_b.multiply(&scale_arr)?;
            let xa = x.matmul(&self.lora_a.t())?;
            let y_lora = xa.matmul(&lora_b_scaled.t())?;

            let y = y_base.add(&y_lora)?;

            if let Some(ref bias) = self.bias {
                Ok(y.add(bias)?)
            } else {
                Ok(y)
            }
        } else {
            // Standard forward: y_base = x @ W.T
            let y_base = x.matmul(&self.weight.t())?;

            // LoRA forward: y_lora = scale * (x @ A.T) @ B.T
            let xa = x.matmul(&self.lora_a.t())?;
            let xab = xa.matmul(&self.lora_b.t())?;
            let scale_arr = Array::from_f32(self.scale);
            let y_lora = xab.multiply(&scale_arr)?;

            // Combined output
            let y = y_base.add(&y_lora)?;

            // Add bias if present
            if let Some(ref bias) = self.bias {
                Ok(y.add(bias)?)
            } else {
                Ok(y)
            }
        }
    }

    /// Ensure optimized cache is built.
    fn ensure_optimized_cache(&mut self) -> Result<(), LoraError> {
        if self.optimized_cache.is_none() {
            let params = OptimizedLoraParams::from_standard(
                &self.weight,
                &self.lora_a,
                &self.lora_b,
                self.scale,
                self.bias.clone(),
            )?;
            self.optimized_cache = Some(params);
        }
        Ok(())
    }

    /// Invalidate the optimized cache (call after gradient updates).
    pub fn invalidate_cache(&mut self) {
        self.optimized_cache = None;
    }

    /// Enable or disable optimized forward pass.
    pub fn set_optimized(&mut self, enabled: bool) {
        self.use_optimized = enabled;
        if !enabled {
            self.optimized_cache = None;
        }
    }

    /// Check if optimized forward is enabled.
    pub fn is_optimized(&self) -> bool {
        self.use_optimized
    }

    /// Forward pass with gradient context for custom autograd.
    ///
    /// This is the unsloth-style custom autograd forward that saves minimal state:
    /// - `x`: Input tensor (for dA computation)
    /// - `x @ A^T`: Intermediate (for dB computation)
    ///
    /// Use this with `backward_with_saved()` for ~50% memory reduction vs standard autodiff.
    ///
    /// # Arguments
    /// * `x` - Input tensor [batch, ..., in_features]
    /// * `ctx` - Gradient context controlling which gradients to compute
    ///
    /// # Returns
    /// Tuple of (output, saved state for backward)
    pub fn forward_with_grad(
        &self,
        x: &Array,
        ctx: &crate::autograd::LoraGradContext,
    ) -> Result<(Array, crate::autograd::LoraForwardSaved), LoraError> {
        crate::autograd::lora_forward_with_grad(
            x,
            &self.weight,
            &self.lora_a,
            &self.lora_b,
            self.scale,
            ctx,
        ).map_err(LoraError::from)
    }

    /// Backward pass using saved state from forward_with_grad.
    ///
    /// Computes gradients for LoRA parameters (and optionally input) using
    /// explicit gradient formulas:
    /// - `dB = scale * (x @ A)^T @ dY`
    /// - `dA = scale * x^T @ (dY @ B^T)`
    /// - `dX = dY @ W + scale * (dY @ B) @ A`
    ///
    /// # Arguments
    /// * `d_output` - Upstream gradient [batch, ..., out_features]
    /// * `saved` - Saved state from forward_with_grad
    ///
    /// # Returns
    /// LoRA gradients (dA, dB, and optionally dX for chain rule)
    pub fn backward_with_saved(
        &self,
        d_output: &Array,
        saved: &crate::autograd::LoraForwardSaved,
    ) -> Result<crate::autograd::LoraGrads, LoraError> {
        crate::autograd::lora_backward(d_output, saved).map_err(LoraError::from)
    }

    /// Apply computed gradients to LoRA parameters using SGD.
    ///
    /// This is a simple gradient descent update:
    /// - `lora_a -= lr * d_lora_a`
    /// - `lora_b -= lr * d_lora_b`
    ///
    /// For more sophisticated optimizers (AdamW, etc.), use the optimizer's update method
    /// with the gradient HashMap from AccumulatedLoraGrads.
    pub fn apply_grads_sgd(
        &mut self,
        grads: &crate::autograd::LoraGrads,
        learning_rate: f32,
    ) -> Result<(), LoraError> {
        let lr = Array::from_f32(learning_rate);
        self.lora_a = self.lora_a.subtract(&grads.d_lora_a.multiply(&lr)?)?;
        self.lora_b = self.lora_b.subtract(&grads.d_lora_b.multiply(&lr)?)?;
        Ok(())
    }

    /// Merge LoRA weights into base weights.
    ///
    /// After merging: `W_merged = W + scale * B @ A`
    /// Invalidates the optimized cache.
    pub fn merge(&mut self) -> Result<(), LoraError> {
        if self.merged {
            return Ok(());
        }

        // W_merged = W + scale * B @ A
        let ba = self.lora_b.matmul(&self.lora_a)?;
        let scale_arr = Array::from_f32(self.scale);
        let delta = ba.multiply(&scale_arr)?;
        let merged_weight = self.weight.add(&delta)?;

        self.weight = merged_weight;
        self.merged = true;
        self.optimized_cache = None;
        Ok(())
    }

    /// Unmerge LoRA weights from base weights.
    ///
    /// After unmerging: `W_original = W_merged - scale * B @ A`
    /// Invalidates the optimized cache.
    pub fn unmerge(&mut self) -> Result<(), LoraError> {
        if !self.merged {
            return Ok(());
        }

        // W_original = W_merged - scale * B @ A
        let ba = self.lora_b.matmul(&self.lora_a)?;
        let scale_arr = Array::from_f32(self.scale);
        let delta = ba.multiply(&scale_arr)?;
        let original_weight = self.weight.subtract(&delta)?;

        self.weight = original_weight;
        self.merged = false;
        self.optimized_cache = None;
        Ok(())
    }

    /// Get the LoRA A parameters (for gradient computation).
    pub fn lora_a_params(&self) -> &Array {
        &self.lora_a
    }

    /// Get the LoRA B parameters (for gradient computation).
    pub fn lora_b_params(&self) -> &Array {
        &self.lora_b
    }

    /// Set the LoRA A parameters.
    /// Invalidates the optimized cache.
    pub fn set_lora_a(&mut self, a: Array) {
        self.lora_a = a;
        self.optimized_cache = None;
    }

    /// Set the LoRA B parameters.
    /// Invalidates the optimized cache.
    pub fn set_lora_b(&mut self, b: Array) {
        self.lora_b = b;
        self.optimized_cache = None;
    }

    /// Get the number of trainable parameters.
    pub fn num_trainable_params(&self) -> usize {
        let lora_a_params = (self.rank * self.in_features) as usize;
        let lora_b_params = (self.out_features * self.rank) as usize;
        lora_a_params + lora_b_params
    }

    /// Get the number of frozen parameters.
    pub fn num_frozen_params(&self) -> usize {
        let weight_params = (self.out_features * self.in_features) as usize;
        let bias_params = if self.use_bias {
            self.out_features as usize
        } else {
            0
        };
        weight_params + bias_params
    }

    /// Get the compression ratio (trainable / frozen).
    pub fn compression_ratio(&self) -> f32 {
        let trainable = self.num_trainable_params() as f32;
        let frozen = self.num_frozen_params() as f32;
        trainable / frozen
    }
}

/// LoRA configuration for patching models.
#[derive(Debug, Clone)]
pub struct LoraLayerConfig {
    /// LoRA rank.
    pub rank: i32,
    /// LoRA alpha.
    pub alpha: f32,
    /// Use RSLoRA scaling.
    pub use_rslora: bool,
    /// Dropout rate (not yet implemented).
    pub dropout: f32,
}

impl LoraLayerConfig {
    /// Create from core LoraConfig.
    pub fn from_core(config: &LoraConfig) -> Self {
        Self {
            rank: config.r as i32,
            alpha: config.alpha,
            use_rslora: config.use_rslora,
            dropout: config.dropout,
        }
    }

    /// Compute the scaling factor.
    pub fn scale(&self) -> f32 {
        if self.use_rslora {
            self.alpha / (self.rank as f32).sqrt()
        } else {
            self.alpha / self.rank as f32
        }
    }
}

impl Default for LoraLayerConfig {
    fn default() -> Self {
        Self {
            rank: 16,
            alpha: 32.0,
            use_rslora: false,
            dropout: 0.0,
        }
    }
}

/// Compute fused LoRA forward pass (functional version).
///
/// Implements: y = x @ W.T + scale * (x @ A.T) @ B.T
///
/// # Arguments
/// * `x` - Input tensor of shape [..., in_features]
/// * `weight` - Base weight matrix of shape [out_features, in_features]
/// * `lora_a` - LoRA A matrix of shape [rank, in_features]
/// * `lora_b` - LoRA B matrix of shape [out_features, rank]
/// * `scale` - LoRA scaling factor (typically alpha / rank)
///
/// # Returns
/// Output tensor of shape [..., out_features]
pub fn fused_lora_forward(
    x: &Array,
    weight: &Array,
    lora_a: &Array,
    lora_b: &Array,
    scale: f32,
) -> Result<Array, LoraError> {
    // Base forward: y_base = x @ W.T
    let y_base = x.matmul(&weight.t())?;

    // LoRA forward: y_lora = scale * (x @ A.T) @ B.T
    let xa = x.matmul(&lora_a.t())?;
    let xab = xa.matmul(&lora_b.t())?;
    let scale_arr = Array::from_f32(scale);
    let y_lora = xab.multiply(&scale_arr)?;

    // Combined output
    Ok(y_base.add(&y_lora)?)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_lora_linear_new() {
        let lora = LoraLinear::new(64, 128, 8, 16.0, false, false).unwrap();

        assert_eq!(lora.in_features, 64);
        assert_eq!(lora.out_features, 128);
        assert_eq!(lora.rank, 8);
        assert!((lora.scale - 2.0).abs() < 1e-6); // alpha / rank = 16 / 8 = 2
        assert!(!lora.merged);
    }

    #[test]
    fn test_lora_linear_forward() {
        let mut lora = LoraLinear::new(32, 64, 4, 8.0, false, false).unwrap();

        let x = mlx_rs::random::normal::<f32>(&[2, 4, 32], None, None, None).unwrap();
        let output = lora.forward(&x).unwrap();

        assert_eq!(output.shape(), &[2, 4, 64]);
    }

    #[test]
    fn test_lora_linear_with_bias() {
        let mut lora = LoraLinear::new(32, 64, 4, 8.0, false, true).unwrap();

        let x = mlx_rs::random::normal::<f32>(&[2, 4, 32], None, None, None).unwrap();
        let output = lora.forward(&x).unwrap();

        assert_eq!(output.shape(), &[2, 4, 64]);
        assert!(lora.bias.is_some());
    }

    #[test]
    fn test_lora_zero_contribution_initial() {
        // With B initialized to zeros, LoRA should have minimal effect initially
        let mut lora = LoraLinear::new(32, 64, 8, 16.0, false, false).unwrap();

        let x = mlx_rs::random::normal::<f32>(&[1, 4, 32], None, None, None).unwrap();
        let output = lora.forward(&x).unwrap();

        // Check base forward without LoRA
        let base_output = x.matmul(&lora.weight.t()).unwrap();

        output.eval().unwrap();
        base_output.eval().unwrap();

        // Outputs should be close since B is zeros
        let diff = output.subtract(&base_output).unwrap();
        let max_diff = diff.abs().unwrap().max(None).unwrap();
        max_diff.eval().unwrap();
        assert!(max_diff.item::<f32>() < 1e-5);
    }

    #[test]
    fn test_lora_merge_unmerge() {
        let mut lora = LoraLinear::new(32, 64, 4, 8.0, false, false).unwrap();

        // Initialize B to non-zero for merge to have effect
        lora.lora_b = mlx_rs::random::normal::<f32>(&[64, 4], None, None, None).unwrap();

        let x = mlx_rs::random::normal::<f32>(&[1, 4, 32], None, None, None).unwrap();

        // Get output before merge
        let output_before = lora.forward(&x).unwrap();
        output_before.eval().unwrap();

        // Merge
        lora.merge().unwrap();
        assert!(lora.merged);

        // Get output after merge
        let output_after = lora.forward(&x).unwrap();
        output_after.eval().unwrap();

        // Outputs should be close
        let diff = output_before.subtract(&output_after).unwrap();
        let max_diff = diff.abs().unwrap().max(None).unwrap();
        max_diff.eval().unwrap();
        assert!(max_diff.item::<f32>() < 1e-4);

        // Unmerge
        lora.unmerge().unwrap();
        assert!(!lora.merged);

        // Get output after unmerge
        let output_unmerged = lora.forward(&x).unwrap();
        output_unmerged.eval().unwrap();

        // Should still be close to original
        let diff2 = output_before.subtract(&output_unmerged).unwrap();
        let max_diff2 = diff2.abs().unwrap().max(None).unwrap();
        max_diff2.eval().unwrap();
        assert!(max_diff2.item::<f32>() < 1e-4);
    }

    #[test]
    fn test_lora_rslora_scaling() {
        let lora_regular = LoraLinear::new(64, 128, 16, 32.0, false, false).unwrap();
        let lora_rs = LoraLinear::new(64, 128, 16, 32.0, true, false).unwrap();

        // Regular: scale = alpha / rank = 32 / 16 = 2.0
        assert!((lora_regular.scale - 2.0).abs() < 1e-6);

        // RSLoRA: scale = alpha / sqrt(rank) = 32 / 4 = 8.0
        assert!((lora_rs.scale - 8.0).abs() < 1e-6);
    }

    #[test]
    fn test_lora_param_count() {
        let lora = LoraLinear::new(512, 1024, 16, 32.0, false, false).unwrap();

        // Trainable: A (16 * 512) + B (1024 * 16) = 8192 + 16384 = 24576
        assert_eq!(lora.num_trainable_params(), 24576);

        // Frozen: W (1024 * 512) = 524288
        assert_eq!(lora.num_frozen_params(), 524288);

        // Compression: 24576 / 524288 â‰ˆ 0.0469
        assert!((lora.compression_ratio() - 0.046875).abs() < 1e-6);
    }

    #[test]
    fn test_fused_lora_forward() {
        let in_features = 32;
        let out_features = 64;
        let rank = 8;
        let scale = 2.0;

        // Create random weights
        let x = mlx_rs::random::normal::<f32>(&[2, 4, in_features], None, None, None).unwrap();
        let weight =
            mlx_rs::random::normal::<f32>(&[out_features, in_features], None, None, None).unwrap();
        let lora_a = mlx_rs::random::normal::<f32>(&[rank, in_features], None, None, None).unwrap();
        let lora_b = mlx_rs::ops::zeros::<f32>(&[out_features, rank]).unwrap();

        let output = fused_lora_forward(&x, &weight, &lora_a, &lora_b, scale).unwrap();

        assert_eq!(output.shape(), &[2, 4, out_features]);
    }
}
